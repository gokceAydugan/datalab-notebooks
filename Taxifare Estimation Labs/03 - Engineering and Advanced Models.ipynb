{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_file = 'data/train-*.csv'\n",
    "valid_data_file = 'data/valid-*.csv'\n",
    "test_data_file = 'data/test-*.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs - Extended\n",
    "\n",
    "1. Define dataset metadata\n",
    "\n",
    "2. Create TF feature columns based on metadata + **Extended Feature Columns**\n",
    "\n",
    "3. Define data input function to populate the features from the data source + **Apply Feature Pre-processing**\n",
    "\n",
    "4. Create experiment: Initialise the Estimator & Evaluation metric + **Wide & Deep Columns for the combined DNN model**\n",
    "\n",
    "5. Run experiment: Supply train data, evaluation data, config, and params\n",
    "\n",
    "6. Evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HEADER = ['pickup_datetime',\n",
    "          'pickup_dayofweek',\n",
    "          'pickup_hour',\n",
    "          'pickup_longitude',\n",
    "          'pickup_latitude',\n",
    "          'dropoff_longitude',\n",
    "          'dropoff_latitude', \n",
    "          'passenger_count',\n",
    "          'fare_amount']\n",
    "\n",
    "\n",
    "DEFAULTS = [['NULL'],['NULL'],[-1], [-74.0], [40.0], [-74.0], [40.7], [-1],[-.1]]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['pickup_longitude', \n",
    "                         'pickup_latitude',\n",
    "                         'dropoff_longitude', \n",
    "                         'dropoff_latitude', \n",
    "                         'passenger_count']\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES_AND_VOCABULARY = {\n",
    "    'pickup_dayofweek' :  ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'],\n",
    "    'pickup_hour': list(range(0,24))\n",
    "}\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + list(CATEGORICAL_FEATURE_NAMES_AND_VOCABULARY.keys())\n",
    "\n",
    "TARGET_NAME = 'fare_amount'\n",
    "\n",
    "UNUSED_FEATURE_NAMES = set(HEADER) - set(FEATURE_NAMES) - set([TARGET_NAME])\n",
    "\n",
    "print(\"Numeric features: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "print(\"Categorical features: {}\".format(list(CATEGORICAL_FEATURE_NAMES_AND_VOCABULARY.keys())))\n",
    "print(\"Target: {}\".format(TARGET_NAME))\n",
    "print(\"Unused features: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input features + extensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extend_feature_columns(feature_columns, hparams):\n",
    "    \n",
    "    #bucketized \n",
    "    buckets = hparams.buckets\n",
    "    lat_buckets = np.linspace(38.0, 42.0, buckets).tolist()\n",
    "    lon_buckets = np.linspace(-76.0, -72.0, buckets).tolist()\n",
    "    \n",
    "    b_plat = tf.feature_column.bucketized_column(feature_columns[FEATURE_NAMES.index(\"pickup_latitude\")], lat_buckets)\n",
    "    b_dlat = tf.feature_column.bucketized_column(feature_columns[FEATURE_NAMES.index(\"dropoff_latitude\")], lat_buckets)\n",
    "    \n",
    "    b_plon = tf.feature_column.bucketized_column(feature_columns[FEATURE_NAMES.index(\"pickup_longitude\")], lon_buckets)\n",
    "    b_dlon = tf.feature_column.bucketized_column(feature_columns[FEATURE_NAMES.index(\"dropoff_longitude\")], lon_buckets)\n",
    "\n",
    "    \n",
    "    #feature crossing (interactions)\n",
    "    ploc = tf.feature_column.crossed_column([b_plat, b_plon], buckets**2)\n",
    "    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], buckets**2)\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], buckets ** 4)\n",
    "    \n",
    "    day_hr =  tf.feature_column.crossed_column([feature_columns[FEATURE_NAMES.index(\"pickup_hour\")], \n",
    "                                                feature_columns[FEATURE_NAMES.index(\"pickup_dayofweek\")]], 24*7)\n",
    "    \n",
    "    \n",
    "    feature_columns = feature_columns + [b_plat,b_dlat,b_plon,dloc,pd_pair,day_hr]\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_feature_columns(hparams):\n",
    "\n",
    "    numeric_columns = list(map(lambda feature_name: tf.feature_column.numeric_column(feature_name),\n",
    "                               NUMERIC_FEATURE_NAMES))\n",
    "\n",
    "    categorical_columns = list(map(lambda item:\n",
    "                                   tf.feature_column.categorical_column_with_vocabulary_list(item[0],item[1]),\n",
    "                                   CATEGORICAL_FEATURE_NAMES_AND_VOCABULARY.items())\n",
    "                               )\n",
    "\n",
    "    feature_columns = numeric_columns + categorical_columns\n",
    "\n",
    "    return extend_feature_columns(feature_columns,hparams)\n",
    "\n",
    "# Test create_feature_columns\n",
    "hparams = tf.contrib.training.HParams(buckets=10)\n",
    "feature_columns = create_feature_columns(hparams) \n",
    "column_names = list(map(lambda column: column.name,feature_columns))\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprossing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_features(features,params):\n",
    "\n",
    "    lat1 = features['pickup_latitude']\n",
    "    lat2 = features['dropoff_latitude']\n",
    "    latdiff = (lat1 - lat2)\n",
    "        \n",
    "    lon1 = features['pickup_longitude']\n",
    "    lon2 = features['dropoff_longitude']\n",
    "    londiff = (lon1 - lon2)\n",
    "\n",
    "    features['longitude_diff'] = latdiff\n",
    "    features['latitude_diff'] = londiff\n",
    "    \n",
    "    dist = tf.sqrt(latdiff*latdiff + londiff*londiff)\n",
    "    features['euclidean_dist'] = dist\n",
    "    \n",
    "    return features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a (scalable) data input function\n",
    "\n",
    "\n",
    "There are couple extensions to this function:\n",
    "\n",
    "\n",
    "* The function can read from multiple csv files in chuncks, hence, the data does not have to fit in the memory.\n",
    "* Incorporate epochs and batch size.\n",
    "* The function applies pre-processing to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def csv_input_fn(filename, hparams,num_epochs=None, batch_size=512):\n",
    "    \n",
    "    \n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    " \n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=num_epochs, shuffle=False)\n",
    "    \n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "    \n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    target = features.pop(TARGET_NAME)\n",
    "    \n",
    "    for feature in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(feature)\n",
    "    \n",
    "    return process_features(features, hparams), target\n",
    "\n",
    "\n",
    "features, target = csv_input_fn(train_data_file,hparams)\n",
    "feature_colum_names = list(features.keys())\n",
    "print(feature_colum_names)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_metrics={\n",
    "    'rmse': tf.contrib.learn.MetricSpec(metric_fn=tf.metrics.root_mean_squared_error)\n",
    "    }\n",
    "\n",
    "def print_evaluation(estimator,hparams):\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    train_metric = estimator.evaluate(input_fn = lambda: csv_input_fn(train_data_file,hparams), \n",
    "                                        steps=1, \n",
    "                                        metrics = evaluation_metrics)\n",
    "\n",
    "    valid_metric = estimator.evaluate(input_fn = lambda: csv_input_fn(valid_data_file,hparams), \n",
    "                                        steps=1, \n",
    "                                        metrics = evaluation_metrics)\n",
    "\n",
    "    test_metric = estimator.evaluate(input_fn = lambda: csv_input_fn(test_data_file,hparams), \n",
    "                                       steps=1, \n",
    "                                       metrics = evaluation_metrics)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"train metric:{}\".format(train_metric))\n",
    "    print(\"valid metric:{}\".format(valid_metric))\n",
    "    print(\"test metric:{}\".format(test_metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the wide and deep features\n",
    "As we are going to use a Wide & Deep Combined NN, we need to define the wide & deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_deep_and_wide_columns(feature_columns, embedding_size):\n",
    "\n",
    "    dense_columns = list(filter(lambda item: \"Numeric\" in str(item.__class__), feature_columns))\n",
    "    sparse_columns = list(filter(lambda item: \"Numeric\" not in str(item.__class__), feature_columns))\n",
    "\n",
    "    embedding_columns =  list(map(lambda sparse_column: \n",
    "                                  tf.feature_column.embedding_column(sparse_column,dimension=embedding_size)\n",
    "                                  ,sparse_columns))\n",
    "\n",
    "    deep_columns = dense_columns + embedding_columns\n",
    "    wide_columns = sparse_columns\n",
    "\n",
    "    return deep_columns, wide_columns\n",
    "\n",
    "  \n",
    "hparams = tf.contrib.training.HParams(embedding_size=10)\n",
    "dense_columns, sparse_columns = get_deep_and_wide_columns(feature_columns,hparams.embedding_size)\n",
    "\n",
    "dense_column_names = list(map(lambda column: column.name,dense_columns))\n",
    "print(dense_column_names)\n",
    "print(\"\")\n",
    "sparse_column_names = list(map(lambda column: column.name,sparse_columns))\n",
    "print(sparse_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and experiement with deep and wide combined NN estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment_dnn_combined_regression(run_config,hparams):\n",
    "\n",
    "    dnn_optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    dnn_combined_estimator = tf.contrib.learn.DNNLinearCombinedRegressor(\n",
    "            linear_feature_columns = sparse_columns,\n",
    "            dnn_feature_columns = dense_columns,\n",
    "            dnn_hidden_units=hparams.hidden_units,\n",
    "            dnn_optimizer=dnn_optimizer,\n",
    "            config = run_config\n",
    "            )\n",
    "    \n",
    "    experiment =  tf.contrib.learn.Experiment(estimator = dnn_combined_estimator, \n",
    "                                     train_steps = hparams.training_steps,\n",
    "                                     train_input_fn = lambda: csv_input_fn(train_data_file,hparams,\n",
    "                                                                           num_epochs=hparams.num_epochs,\n",
    "                                                                           batch_size = hparams.batch_size\n",
    "                                                                          ), \n",
    "                                     eval_input_fn =lambda: csv_input_fn(valid_data_file,hparams),\n",
    "                                     eval_metrics = evaluation_metrics\n",
    "                                    )\n",
    "    return experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params and run experiemnt - DNN Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set params\n",
    "hparams  = tf.contrib.training.HParams(training_steps=None,\n",
    "                                       num_epochs = 1000,\n",
    "                                       batch_size = 1300,\n",
    "                                       embedding_size = 8,\n",
    "                                       buckets = 8,\n",
    "                                       hidden_units=[64, 32, 16])\n",
    "\n",
    "\n",
    "model_dir = \"trained_models/dnn_combined_regression_model\"\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "tf.contrib.learn.learn_runner.run(experiment_fn = experiment_dnn_combined_regression, \n",
    "                               run_config = run_config,\n",
    "                               schedule=\"train_and_evaluate\",\n",
    "                               hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate trained model - DNN Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn_combined_model = tf.contrib.learn.DNNLinearCombinedRegressor(\n",
    "            linear_feature_columns = sparse_columns,\n",
    "            dnn_feature_columns = dense_columns,\n",
    "            dnn_hidden_units=hparams.hidden_units,\n",
    "            config = run_config\n",
    "            )\n",
    "\n",
    "print_evaluation(dnn_combined_model,hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame({\n",
    "              'Method' : pd.Series(['Basline', 'Linear Reg', 'DNN', ' Comb DNN + Feature Eng', '----', '-----']),\n",
    "              'RMSE': pd.Series([8.89, 11.15, 14.94, 8.1, 0.0, .0]) })\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(data=df, x='Method', y='RMSE')\n",
    "ax.set_ylabel('RMSE (dollars)')\n",
    "ax.set_xlabel('Method')\n",
    "plt.plot(np.linspace(-20,120,1000), [8.1]*1000, 'b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
