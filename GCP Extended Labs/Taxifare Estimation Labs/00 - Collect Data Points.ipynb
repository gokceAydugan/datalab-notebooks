{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from google.cloud import pubsub\n",
    "import json\n",
    "import apache_beam as beam\n",
    "import os\n",
    "print(beam.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "RUNNER = \"Dataflow\"\n",
    "PROJECT = 'ksalama-gcp-playground'\n",
    "DATASET = 'playground_ds'\n",
    "TABLE = 'taxi_trips'\n",
    "STG_BUCKET = 'stagging-ksalama-gcs-cloudml'\n",
    "REGION = 'europe-west1'\n",
    "TOPIC = 'taxi-trips'\n",
    "SUBSCRIPTION='taxi-trips-sub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Dataflow Stream Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubsub_subscription = \"projects/{}/subscriptions/{}\".format(PROJECT,SUBSCRIPTION)\n",
    "pubsub_topic = \"projects/{}/topics/{}\".format(PROJECT,TOPIC)\n",
    "\n",
    "print(pubsub_subscription)\n",
    "print(DATASET,TABLE)\n",
    "\n",
    "def process_events(message):\n",
    "    data_row = message.data\n",
    "    #preform all yor data processing, transformation, calling external APIs, etc.\n",
    "    return data_row\n",
    "  \n",
    "def run_taxi_trips_pipeline():\n",
    "    \n",
    "    job_name = 'ingest-taxi-trips-{}'.format(datetime.datetime.now().strftime('%y%m%d-%H%M%S'))\n",
    "    print 'Launching Dataflow job {}'.format(job_name)\n",
    "    print 'Check the Dataflow jobs on Google Cloud Console...'\n",
    "\n",
    "    STG_DIR = 'gs://{}/taxi-fare'.format(STG_BUCKET)\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(STG_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(STG_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'streaming': True,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "      }\n",
    "\n",
    "\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    \n",
    "    pipeline = beam.Pipeline(RUNNER, options=opts)\n",
    "      \n",
    "    (\n",
    "      pipeline | 'Read data from PubSub' >> beam.io.ReadStringsFromPubSub(subscription=pubsub_subscription) \n",
    "               | 'Process message' >> beam.Map(process_events) # filter, window, group, aggregate \n",
    "               | 'Write to BigQuery' >> beam.io.WriteToBigQuery(project=PROJECT, dataset=DATASET,table=TABLE)\n",
    "    )\n",
    "\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_taxi_trips_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_header_names = ['trip_datetime', 'pickup_dayofweek', 'pickup_hour','pickup_lon', 'pickup_lat', 'dropoff_lon', 'dropoff_lat', 'passenger_count', 'fare_amount']\n",
    "header_names = ['trip_datetime', 'pickup_lon', 'pickup_lat', 'dropoff_lon', 'dropoff_lat', 'passenger_count', 'fare_amount']\n",
    "\n",
    "dataset = pd.read_csv('data/train-data.csv', header=None, names=csv_header_names)[header_names]\n",
    "def get_data_points(count=10):\n",
    "  \n",
    "    data_points = []\n",
    "  \n",
    "    instances = dataset.sample(n=count).values\n",
    "  \n",
    "    for row in instances:\n",
    "        data_point = dict()\n",
    "    \n",
    "    for i in range(len(row)):\n",
    "        data_point[header_names[i]] = row[i]\n",
    "     \n",
    "    data_points.append(data_point)\n",
    "      \n",
    "    return data_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Data Points to Pub/Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "iterations = 10\n",
    "sleep_time = 1\n",
    "\n",
    "client = pubsub.Client()\n",
    "topic = client.topic(TOPIC)\n",
    "\n",
    "if not topic.exists():\n",
    "    print ('Creating pub/sub topic {}...'.format(TOPIC))\n",
    "    topic.create()\n",
    "\n",
    "print ('Pub/sub topic {} is up and running'.format(TOPIC))\n",
    "print(\"\")\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    data_points = get_data_points(batch_size)\n",
    "    \n",
    "    for data_point in data_points:\n",
    "\n",
    "        source_id = str(abs(hash(str(data_point))) % (10 ** 10))\n",
    "        source_timestamp = datetime.datetime.now().strftime(TIME_FORMAT)\n",
    "        message = json.dumps(data_point)\n",
    "        topic.publish(message=message, source_id = source_id, source_timestamp=source_timestamp)\n",
    "\n",
    "    print(\"Batch {} was sent. Last Message was: {}\".format(i, message))\n",
    "    print(\"\")\n",
    "\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume PubSub Topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = pubsub.Client()\n",
    "topic = client.topic(TOPIC)\n",
    "subscription = topic.subscription(SUBSCRIPTION)\n",
    "message = subscription.pull()\n",
    "\n",
    "# print(message[0][1].source_timestamp)\n",
    "print(\"source_id\", message[0][1].attributes[\"source_id\"])\n",
    "print(\"source_timestamp:\", message[0][1].attributes[\"source_timestamp\"])\n",
    "print(\"\")\n",
    "print(message[0][1].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
