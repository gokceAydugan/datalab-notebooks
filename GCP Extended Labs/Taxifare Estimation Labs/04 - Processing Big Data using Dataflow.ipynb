{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling training data\n",
    "\n",
    "In order to train an ML model on large dataset, the dataset needs to be shared into csv files and stored on Google Cloud Storage (GCS).\n",
    "\n",
    "Now, we are going to export the data (1.5M training records & 500k validation records) from BigQuery to GCS using Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "print(beam.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RUNNER = \"Dataflow\"\n",
    "PROJECT = 'ksalama-gcp-playground'\n",
    "BUCKET = 'ksalama-gcs-cloudml'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare BigQuery query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALID_ROW_COUNT = 75280784\n",
    "\n",
    "sql = \"\\\n",
    "SELECT \\\n",
    "    pickup_datetime, \\\n",
    "    EXTRACT(DAYOFWEEK FROM pickup_datetime) AS pickup_dayofweek, \\\n",
    "    EXTRACT(HOUR FROM pickup_datetime) AS pickup_hour, \\\n",
    "    pickup_longitude, \\\n",
    "    pickup_latitude, \\\n",
    "    dropoff_longitude, \\\n",
    "    dropoff_latitude, \\\n",
    "    passenger_count, \\\n",
    "    tolls_amount + fare_amount AS fare_amount \\\n",
    "FROM `nyc-tlc.yellow.trips` \\\n",
    "WHERE \\\n",
    "    trip_distance > 0 \\\n",
    "AND fare_amount >= 2.5 \\\n",
    "AND pickup_longitude  > -78 \\\n",
    "AND pickup_longitude  < -70 \\\n",
    "AND dropoff_longitude  > -78 \\\n",
    "AND dropoff_longitude  < -70 \\\n",
    "AND pickup_latitude  > 37 \\\n",
    "AND pickup_latitude < 45 \\\n",
    "AND dropoff_latitude  > 37 \\\n",
    "AND dropoff_latitude  < 45 \\\n",
    "AND passenger_count  > 0 \\\n",
    "AND EXTRACT(YEAR FROM pickup_datetime) = 2015 \\\n",
    "AND MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))),@STEP_SIZE) = @PHASE \\\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataflow pipeline logic to export data from BigQuery to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_csv(row_dictionary):\n",
    "    import copy\n",
    "    days = ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    \n",
    "    HEADER = ['pickup_datetime',\n",
    "            'pickup_dayofweek',\n",
    "            'pickup_hour',\n",
    "            'pickup_longitude',\n",
    "            'pickup_latitude',\n",
    "            'dropoff_longitude',\n",
    "            'dropoff_latitude', \n",
    "            'passenger_count',\n",
    "            'fare_amount']\n",
    "    \n",
    "    result = copy.deepcopy(row_dictionary)\n",
    "    result['pickup_dayofweek'] = days[result['pickup_dayofweek']]\n",
    "    return ','.join([str(result[k]) for k in HEADER])\n",
    "\n",
    "def run_pipeline(phase,sample_size):\n",
    "\n",
    "    phase_string = \"training\" if phase == 2 else \"validation\"\n",
    "    print(\"Phase:{}\".format(phase_string))\n",
    "    print(\"Sample Size:{}\".format(sample_size))\n",
    "\n",
    "    job_name = 'export-taxi-{}-data-{}'.format(phase_string,datetime.datetime.now().strftime('%y%m%d-%H%M%S'))\n",
    "    print 'Launching Dataflow job {}'.format(job_name)\n",
    "    print 'Check the Dataflow jobs on Google Cloud Console...'\n",
    "\n",
    "    OUTPUT_DIR = 'gs://{0}/data/nyc-taxifare/big'.format(BUCKET)\n",
    "\n",
    "    options = {\n",
    "      'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "      'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "      'job_name': job_name,\n",
    "      'project': PROJECT,\n",
    "      'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "      'no_save_main_session': True\n",
    "    }\n",
    "\n",
    "\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    p = beam.Pipeline(RUNNER, options=opts)\n",
    "\n",
    "    step_size = int(VALID_ROW_COUNT / sample_size) - 1\n",
    "\n",
    "    query = sql.replace('@STEP_SIZE', str(step_size)).replace('@PHASE', str(phase))\n",
    "\n",
    "    sink = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase_string))\n",
    "\n",
    "    (\n",
    "      p | 'Read data from BigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n",
    "        | 'Convert to csv' >> beam.Map(to_csv) # map, filter, group, windowing, etc.\n",
    "        | 'Write csv file to GCS' >> beam.io.Write(beam.io.WriteToText(sink))\n",
    "    )\n",
    "\n",
    "    p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Dataflow pipelines to process data to GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = 1500000\n",
    "valid_size = 500000\n",
    "\n",
    "run_pipeline(2, train_size)\n",
    "\n",
    "run_pipeline(3, valid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
