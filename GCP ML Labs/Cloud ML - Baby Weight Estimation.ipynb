{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured data prediction using Cloud ML Engine \n",
    "\n",
    "This notebook illustrates:\n",
    "\n",
    "1. Exploring a BigQuery dataset using Datalab\n",
    "2. Creating datasets for Machine Learning using Dataflow\n",
    "3. Creating a model using the high-level Estimator API \n",
    "4. Training on Cloud ML Engine\n",
    "5. Deploying the model\n",
    "6. Predicting with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUCKET = 'ksalama-gcs-cloudml'\n",
    "PROJECT = 'ksalama-gcp-playground'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gcs_data_dir = 'gs://{0}/data/babyweight/'.format(BUCKET)\n",
    "gcs_model_dir = 'gs://{0}/ml-models/babyweight/'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m rm -rf gs://ksalama-gcs-cloudml/ml-models/babyweight/*\n",
    "gsutil -m rm -rf gs://ksalama-gcs-cloudml/data/babyweight/datasets/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query data in BigQuery\n",
    "\n",
    "The data is natality data (record of births in the US). My goal is to predict the baby's weight given a number of factors about the pregnancy and the baby's mother.  Later, we will want to split the data into training and eval datasets. The hash of the year-month will be used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name data\n",
    "\n",
    "SELECT\n",
    "  CAST(mother_race AS string) race_index,\n",
    "  AVG(weight_pounds) avg_weight,\n",
    "  COUNT(weight_pounds) instance_Count\n",
    "FROM\n",
    "  `publicdata.samples.natality`\n",
    "WHERE \n",
    "    year > 2000\n",
    "AND weight_pounds > 0\n",
    "AND mother_age > 0\n",
    "AND plurality > 0\n",
    "AND gestation_weeks > 0\n",
    "AND month > 0\n",
    "AND mother_race is not null\n",
    "GROUP BY\n",
    "  mother_race\n",
    "ORDER BY\n",
    "  avg_weight DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise with Datalab commands \n",
    "http://googledatalab.github.io/pydatalab/google.datalab%20Commands.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%chart columns --data data --fields race_index, avg_weight\n",
    "title: Mother Race Index vs Average Baby Weight\n",
    "height: 400\n",
    "width: 900\n",
    "hAxis:\n",
    "  title: Race Index\n",
    "vAxis:\n",
    "  title: Average Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from BigQuery as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql --module query \n",
    "\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  mother_race,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  mother_married,\n",
    "  ever_born,\n",
    "  cigarette_use,\n",
    "  alcohol_use\n",
    "FROM\n",
    "  `publicdata.samples.natality`\n",
    "WHERE \n",
    "    year > 2000\n",
    "AND weight_pounds > 0\n",
    "AND mother_age > 0\n",
    "AND plurality > 0\n",
    "AND gestation_weeks > 0\n",
    "AND month > 0\n",
    "AND\n",
    "  MOD(ABS(FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING)))), 2) = 0\n",
    "LIMIT $STEP_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import sys\n",
    "data = bq.Query(query, STEP_SIZE = step_size).to_dataframe(dialect='standard')\n",
    "print('Row count:{}'.format(len(data)))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore & Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "#plt.figure(figsize=(45, 25))\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# Baby Weight Distribution\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Baby Weight Histogram\")\n",
    "plt.hist(data.weight_pounds, bins=150)\n",
    "#plt.axis([0, 50, 0, 3500])\n",
    "plt.xlabel(\"Baby Weight Ranges\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# ---------------------------\n",
    "\n",
    "# Mother Age vs Baby Weight\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Mother Age vs Baby Weight\")\n",
    "plt.scatter(data.mother_age,data.weight_pounds)\n",
    "plt.xlabel(\"Mother Age\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "# ---------------------------\n",
    "\n",
    "# Gestation Weeks vs Baby Weight\n",
    "plt.subplot(2,3,3)\n",
    "fit = np.polyfit(data.gestation_weeks,data.weight_pounds, deg=1)\n",
    "plt.plot(data.gestation_weeks, fit[0] * data.gestation_weeks + fit[1], color='red')\n",
    "plt.scatter(data.gestation_weeks, data.weight_pounds)\n",
    "plt.xlabel(\"Gestation Weeks\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "\n",
    "#---------------------------\n",
    "\n",
    "# Is Male vs Baby Weight Boxplot\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Is Male vs Baby Weight\")\n",
    "\n",
    "is_male_values = list(data.is_male.value_counts().index.values)\n",
    "is_male_data = []\n",
    "for i in is_male_values:\n",
    "    is_male_data = is_male_data + [data.weight_pounds[data.is_male == i].values]\n",
    "\n",
    "plt.boxplot(is_male_data)\n",
    "plt.axis([0, 3, 4, 11])\n",
    "plt.xlabel(\"Is Male\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "# ---------------------------\n",
    "\n",
    "# Mother Race vs Baby Weight Boxplot\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Mother Race vs Baby Weight\")\n",
    "\n",
    "race_values = list(data.mother_race.value_counts().index.values)\n",
    "race_data = []\n",
    "for i in race_values:\n",
    "    race_data = race_data + [data.weight_pounds[data.mother_race == i].values]\n",
    "\n",
    "plt.boxplot(race_data)\n",
    "plt.axis([0, 16, 4, 11])\n",
    "plt.xlabel(\"Mother Race\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "\n",
    "# # ---------------------------\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"Alcohol & Cigarette Use\")\n",
    "\n",
    "alch_use_values = list(data.alcohol_use.value_counts().index.values)\n",
    "cig_use_values = list(data.cigarette_use.value_counts().index.values)\n",
    "\n",
    "use_data = []\n",
    "labels = []\n",
    "\n",
    "for i in alch_use_values:\n",
    "    for j in cig_use_values:\n",
    "        labels = labels + ['alch-use:{} & cig-use:{}'.format(i,j)]\n",
    "        condition = (data.alcohol_use == i) & (data.cigarette_use == j)\n",
    "        values = data.weight_pounds[condition].values\n",
    "        if (len(values) > 0):\n",
    "            use_data = use_data + [len(values)]\n",
    "\n",
    "plt.pie(use_data)\n",
    "plt.legend(labels, loc=\"lower center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Weight as a Baseline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_weight = data.weight_pounds.mean()\n",
    "print(\"Average Weight: {}\".format(round(avg_weight,3)))\n",
    "rmse = np.sqrt(data.weight_pounds.map(lambda value: (value-avg_weight)**2).mean())\n",
    "print(\"RMSE: {}\".format(round(rmse,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML dataset using Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import datetime\n",
    "\n",
    "dataset_size = 100000\n",
    "train_size = dataset_size * 0.7\n",
    "eval_size = dataset_size * 0.3\n",
    "\n",
    "out_dir = gcs_data_dir\n",
    "\n",
    "print(out_dir)\n",
    "\n",
    "def to_csv(rowdict):\n",
    "    # pull columns from BQ and create a line\n",
    "    import hashlib\n",
    "    import copy\n",
    "    CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use'.split(',')\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = copy.deepcopy(rowdict)\n",
    "    if 'mother_race' in rowdict and rowdict['mother_race'] in races:\n",
    "        result['mother_race'] = races[rowdict['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "    \n",
    "    data = ','.join([str(result[k]) if k in result else 'None' for k in CSV_COLUMNS])\n",
    "    key = hashlib.sha224(data).hexdigest()  # hash the columns to form a key\n",
    "    return str('{},{}'.format(data, key))\n",
    "  \n",
    "def run_pipeline():\n",
    "    \n",
    "    job_name = 'preprocess-babyweight-data' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "\n",
    "    options = {\n",
    "        'staging_location': os.path.join(out_dir, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(out_dir, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': PROJECT,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "  \n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "          weight_pounds,\n",
    "          is_male,\n",
    "          mother_age,\n",
    "          mother_race,\n",
    "          plurality,\n",
    "          gestation_weeks,\n",
    "          mother_married,\n",
    "          ever_born,\n",
    "          cigarette_use,\n",
    "          alcohol_use,\n",
    "          FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
    "        FROM\n",
    "          publicdata.samples.natality\n",
    "        WHERE year > 2000\n",
    "        AND weight_pounds > 0\n",
    "        AND mother_age > 0\n",
    "        AND plurality > 0\n",
    "        AND gestation_weeks > 0\n",
    "        AND month > 0\n",
    "    \"\"\"\n",
    "  \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    RUNNER = 'DataflowRunner'\n",
    "  \n",
    "    p = beam.Pipeline(RUNNER, options=opts)\n",
    "    \n",
    "    for step in ['train', 'eval']:\n",
    "        if step == 'train':\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(hashmonth,4) < 3 LIMIT {}'.format(query,int(train_size))\n",
    "        else:\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(hashmonth,4) = 3 LIMIT {}'.format(query,int(eval_size))\n",
    "\n",
    "        (p \n",
    "           | '{} - Read from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "           | '{} - Process to CSV'.format(step) >> beam.Map(to_csv)\n",
    "           | '{} - Write to GCS '.format(step) >> beam.io.Write(beam.io.WriteToText(os.path.join(out_dir, '{}-data.csv'.format(step))))\n",
    "        )\n",
    "    \n",
    "   \n",
    "    job = p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Dataflow Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cp gs://ksalama-gcs-cloudml/data/babyweight/train-data.csv data/babyweight\n",
    "gsutil cp gs://ksalama-gcs-cloudml/data/babyweight/eval-data.csv data/babyweight\n",
    "\n",
    "echo \" \"\n",
    "ls data/babyweight\n",
    "\n",
    "echo \" \"\n",
    "head -3 data/babyweight/train-data.csv\n",
    "\n",
    "echo \" \"\n",
    "wc -l data/babyweight/train-data.csv\n",
    "wc -l data/babyweight/eval-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow DNN Regressor using Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "\n",
    "1. Define dataset metadata\n",
    "\n",
    "2. Define data **input function** to populate the features from the data source + ** Feature Processing**\n",
    "\n",
    "3. Create **feature columns** based on metadata + **Extended Feature Columns**\n",
    "\n",
    "4. Define and **Estimator** (LinearCombinedDNNRegressor), given the feature columns and the parameters\n",
    "\n",
    "5. Create experiment: Initialise the Estimator  \n",
    "\n",
    "6. Run experiment: Supply train data, evaluation data, config, and params\n",
    "\n",
    "7. Evaluate the trained model on the evaluation set\n",
    "\n",
    "<img src=\"images/exp-api.png\" width=\"1400\" hight=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'babyweight_estimator'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = False\n",
    "EXTEND_FEATURES_COLUMS = True\n",
    "\n",
    "train_filename = 'data/babyweight/train-data.csv*'\n",
    "eval_filename = 'data/babyweight/eval-data.csv*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Define Metadata \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "HEADER_DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], [0.0], ['null'], ['null'], ['null'], ['nokey']]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['mother_age', 'plurality', 'gestation_weeks']\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'is_male': ['True', 'False'],\n",
    "                                             'mother_race': ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                                                             'Japanese', 'Hawaiian', 'Filipino', 'Unknown',\n",
    "                                                             'Asian Indian', 'Korean', 'Samaon', 'Vietnamese'],\n",
    "                                             'mother_married': ['True', 'False'],\n",
    "                                             'cigarette_use': ['True', 'False', 'None'],\n",
    "                                             'alcohol_use': ['True', 'False', 'None']\n",
    "                                             \n",
    "                                            }\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TARGET_NAME = 'weight_pounds'\n",
    "\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})\n",
    "\n",
    "\n",
    "print(\"Header: {}\".format(HEADER))\n",
    "print(\"Numeric Features: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "print(\"Categorical Features: {}\".format(CATEGORICAL_FEATURE_NAMES))\n",
    "print(\"Target: {}\".format(TARGET_NAME))\n",
    "print(\"Unused Features: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Define Data  Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_features(features):\n",
    "    return features\n",
    "\n",
    "def parse_csv_row(csv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)\n",
    "\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    for column in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(column)\n",
    "    \n",
    "    target = features.pop(TARGET_NAME)\n",
    "    \n",
    "\n",
    "    return features, target\n",
    "\n",
    "\n",
    "def csv_input_fn(filename, mode=tf.estimator.ModeKeys.EVAL, num_epochs=None, batch_size=512):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file(s): {}\".format(filename))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    " \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=num_epochs, shuffle=False)\n",
    "    \n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=HEADER_DEFAULTS)\n",
    "    \n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    for column in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(column)\n",
    "    \n",
    "    target = features.pop(TARGET_NAME)   \n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        features = process_features(features)\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = csv_input_fn(filename=['/data/babyweight/train-data.csv'])\n",
    "print(\"Feature read from CSV: {}\".format(list(features.keys())))\n",
    "print(\"Feature read from CSV: {}\".format(features))\n",
    "print(\"Target read from CSV: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Create Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extend_feature_columns(feature_columns, hparams):\n",
    "    \n",
    "    feature_columns['cigarette_use_X_alcohol_use'] = tf.feature_column.crossed_column(\n",
    "        keys=[feature_columns['cigarette_use'], feature_columns['alcohol_use']], hash_bucket_size=9)\n",
    "    \n",
    "    feature_columns['mother_race_embedded'] = tf.feature_column.embedding_column(\n",
    "        feature_columns['mother_race'], hparams.embedding_size)\n",
    "    \n",
    "    return feature_columns\n",
    "\n",
    "def get_feature_columns(hparams):\n",
    "    \n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = []\n",
    "    all_numeric_feature_names = NUMERIC_FEATURE_NAMES\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        all_numeric_feature_names += CONSTRUCTED_NUMERIC_FEATURES_NAMES\n",
    "\n",
    "    numeric_columns = {feature_name: tf.feature_column.numeric_column(feature_name)\n",
    "                       for feature_name in all_numeric_feature_names}\n",
    "\n",
    "    categorical_column_with_vocabulary = \\\n",
    "        {item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "        \n",
    "    feature_columns = {}\n",
    "\n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "        \n",
    "    if EXTEND_FEATURES_COLUMS:\n",
    "        feature_columns = extend_feature_columns(feature_columns, hparams)\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Define a Regression Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deep_and_wide_columns(hparams):\n",
    "    \n",
    "    feature_columns = list(get_feature_columns(hparams).values())\n",
    "    \n",
    "    dense_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._NumericColumn) |\n",
    "                              isinstance(column, feature_column._EmbeddingColumn),\n",
    "               feature_columns\n",
    "        )\n",
    "    )\n",
    "\n",
    "    categorical_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._VocabularyListCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._BucketizedColumn),\n",
    "                   feature_columns)\n",
    "    )\n",
    "    \n",
    "    sparse_columns = list(\n",
    "        filter(lambda column: isinstance(column,feature_column._HashedCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._CrossedColumn),\n",
    "               feature_columns)\n",
    "    )\n",
    "\n",
    "    indicator_columns = list(\n",
    "            map(lambda column: tf.feature_column.indicator_column(column),\n",
    "                categorical_columns)\n",
    "    )\n",
    "    \n",
    "    deep_feature_columns = dense_columns #+ indicator_columns\n",
    "    wide_feature_columns = categorical_columns + sparse_columns\n",
    "\n",
    "    return wide_feature_columns, deep_feature_columns\n",
    "\n",
    "\n",
    "def create_DNNLinearCombinedRegressor(run_config, hparams, print_desc=True):\n",
    "  \n",
    "    wide_feature_columns, deep_feature_columns = get_deep_and_wide_columns(hparams)\n",
    "\n",
    "    dnn_optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    estimator = tf.contrib.learn.DNNLinearCombinedRegressor(\n",
    "                linear_feature_columns = wide_feature_columns,\n",
    "                dnn_feature_columns = deep_feature_columns,\n",
    "                dnn_optimizer=dnn_optimizer,\n",
    "                dnn_hidden_units=hparams.hidden_units,\n",
    "                config = run_config\n",
    "                )\n",
    "\n",
    "    if print_desc:\n",
    "        print(\"\")\n",
    "        print(\"*Estimator Type:\")\n",
    "        print(\"================\")\n",
    "        print(type(estimator))\n",
    "        print(\"\")\n",
    "        print(\"*deep columns:\")\n",
    "        print(\"==============\")\n",
    "        print(deep_feature_columns)\n",
    "        print(\"\")\n",
    "        print(\"wide columns:\")\n",
    "        print(\"=============\")\n",
    "        print(wide_feature_columns)\n",
    "        print(\"\")\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_experiment(run_config,hparams):\n",
    "    \n",
    "    train_input_fn = lambda: csv_input_fn(train_filename,\n",
    "                                          mode= tf.estimator.ModeKeys.TRAIN,\n",
    "                                          num_epochs=hparams.num_epochs,\n",
    "                                          batch_size = hparams.batch_size)\n",
    "    \n",
    "    eval_input_fn = lambda: csv_input_fn(eval_filename,\n",
    "                                          mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                          num_epochs=1,\n",
    "                                          batch_size = hparams.batch_size)\n",
    "    \n",
    "    estimator = create_DNNLinearCombinedRegressor(run_config, hparams)\n",
    "    \n",
    "    evaluation_metrics={\n",
    "    'rmse': tf.contrib.learn.MetricSpec(metric_fn=tf.metrics.root_mean_squared_error)\n",
    "    }\n",
    "    \n",
    "    experiment =  tf.contrib.learn.Experiment(estimator = estimator, \n",
    "                                     train_steps = hparams.training_steps,\n",
    "                                     train_input_fn = train_input_fn, \n",
    "                                     eval_input_fn = eval_input_fn,\n",
    "                                     eval_steps = None,\n",
    "                                     eval_metrics = evaluation_metrics\n",
    "                                    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = \"trained_models/babyweight/{}\".format(MODEL_NAME)\n",
    "\n",
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(local_model_dir)\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to stop TensorBoard\n",
    "# TensorBoard().stop(23002)\n",
    "# print('stopped TensorBoard')\n",
    "# TensorBoard().list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Run Local Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Set params\n",
    "hparams  = tf.contrib.training.HParams(training_steps=None,\n",
    "                                       num_epochs = 10,\n",
    "                                       batch_size = 1000,\n",
    "                                       hidden_units=[32, 16],\n",
    "                                       embedding_size = 4\n",
    "                                      )\n",
    "\n",
    "\n",
    "if not RESUME_TRAINING:\n",
    "    shutil.rmtree(local_model_dir, ignore_errors=True)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    model_dir=local_model_dir\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.contrib.learn.learn_runner.run(experiment_fn = create_experiment, \n",
    "                                  run_config = run_config,\n",
    "                                  schedule=\"train_and_evaluate\",\n",
    "                                  hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m cp -r gs://ksalama-gcs-cloudml/ml-packages/babyweight ml-packages\n",
    "ls ml-packages/babyweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=BASIC # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-data.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-data.csv\n",
    "MODEL_DIR=gs://${BUCKET}/trained-models/${MODEL_NAME}\n",
    "\n",
    "#remove model directory, if you don't want to resume training, or if you have changed the model structure\n",
    "#gsutil -m rm -r ${MODEL_DIR}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.2 \\\n",
    "        --region=${REGION} \\\n",
    "        --scale-tier=${TIER} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --learning-rate=0.001 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on Cloud ML Engine + GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=BASIC_GPU # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-data.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-data.csv\n",
    "MODEL_DIR=gs://${BUCKET}/trained-models/${MODEL_NAME}_${TIER}\n",
    "\n",
    "#remove model directory, if you don't want to resume training, or if you have changed the model structure\n",
    "#gsutil -m rm -r ${MODEL_DIR}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.2 \\\n",
    "        --region=${REGION} \\\n",
    "        --scale-tier=${TIER} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=10 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --learning-rate=0.01 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model on Cloud ML Engine + Custom GPUs Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=CUSTOM # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-data.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-data.csv\n",
    "MODEL_DIR=gs://${BUCKET}/trained-models/${MODEL_NAME}_${TIER}\n",
    "\n",
    "#remove model directory, if you don't want to resume training, or if you have changed the model structure\n",
    "#gsutil -m rm -r ${MODEL_DIR}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.2 \\\n",
    "        --region=${REGION} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        --config=ml-packages/babyweight/custom.yaml \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --learning-rate=0.001 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters Tuning on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=ml-packages/babyweight/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-data.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-data.csv\n",
    "MODEL_DIR=gs://${BUCKET}/trained-models/${MODEL_NAME}_tune\n",
    "\n",
    "#remove model directory, if you don't want to resume training, or if you have changed the model structure\n",
    "#gsutil -m rm -r ${MODEL_DIR}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=tune_${MODEL_NAME}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.2 \\\n",
    "        --region=${REGION} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        --config=ml-packages/babyweight/hyperparams.yaml \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REGION=europe-west1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "\n",
    "#MODEL_BINARIES=$(gsutil ls gs://${BUCKET}/trained-models/${MODEL_NAME}/export/Servo | tail -1)\n",
    "\n",
    "#gsutil ls ${MODEL_BINARIES}\n",
    "\n",
    "# delete model version\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model=${MODEL_NAME}\n",
    "\n",
    "# delete model\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "\n",
    "# deploy model to GCP\n",
    "#gcloud ml-engine models create ${MODEL_NAME} --regions=${REGION}\n",
    "\n",
    "#deploy model version\n",
    "#gcloud ml-engine versions create ${MODEL_VERSION} --model=${MODEL_NAME} --origin=${MODEL_BINARIES} --runtime-version=1.2\n",
    "\n",
    "#echo  ${MODEL_NAME} ${MODEL_VERSION} \n",
    "# invoke deployed model to make prediction given new data instances\n",
    "#gcloud ml-engine predict --model=${MODEL_NAME} --version=${MODEL_VERSION} --json-instances=data/babyweight/new-data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume the Model as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "def estimate(project, model_name, version, instances):\n",
    "\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    api = discovery.build('ml', 'v1', credentials=credentials,\n",
    "                discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "    request_data = {'instances': instances}\n",
    "\n",
    "    model_url = 'projects/{}/models/{}/versions/{}'.format(project, model_name, version)\n",
    "    response = api.projects().predict(body=request_data, name=model_url).execute()\n",
    "\n",
    "    estimates = list(map(lambda item: round(item[\"scores\"],2)\n",
    "        ,response[\"predictions\"]\n",
    "    ))\n",
    "\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='ksalama-gcp-playground'\n",
    "MODEL_NAME='babyweight_estimator'\n",
    "VERSION='v2'\n",
    "\n",
    "instances =  [\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'False',\n",
    "        'mother_age': 29.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 38,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 2.0,\n",
    "        'gestation_weeks': 37,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      }\n",
    "  ]\n",
    "\n",
    "estimates = estimate(instances=instances\n",
    "                     ,project=PROJECT\n",
    "                     ,model_name=MODEL_NAME\n",
    "                     ,version=VERSION)\n",
    "\n",
    "print(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the end ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
