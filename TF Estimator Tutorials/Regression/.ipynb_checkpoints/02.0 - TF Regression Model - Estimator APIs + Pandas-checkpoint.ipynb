{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidsalama/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "1. Define dataset **metadata**\n",
    "2. Define **data input function** to read the data from Pandas dataframe + **apply pre-processing**\n",
    "3. Create TF **feature columns** based on metadata + **extended feature columns**\n",
    "4. Instantiate an **estimator** with the required **feature columns & parameters**\n",
    "5. **Train** estimator using training data\n",
    "6. **Evaluate** estimator using test data\n",
    "7. **Save & Serve** the estimator\n",
    "8.  Perform **predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_file = 'data/train-data.csv'\n",
    "valid_data_file = 'data/valid-data.csv'\n",
    "test_data_file = 'data/test-data.csv'\n",
    "\n",
    "model_name = 'reg-model-01'\n",
    "\n",
    "resume = False\n",
    "train = True\n",
    "multi_threading = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['key', 'x', 'y', 'alpha', 'beta', 'target']\n",
      "Numeric Features: ['x', 'y']\n",
      "Categorical Features: ['alpha', 'beta']\n",
      "Target: target\n",
      "Unused Features: ['key']\n"
     ]
    }
   ],
   "source": [
    "HEADER = ['key','x','y','alpha','beta','target']\n",
    "HEADER_DEFAULTS = [[0], [0.0], [0.0], ['NA'], ['NA'], [0.0]]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = ['x', 'y']  \n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'alpha':['ax01', 'ax02'], 'beta':['bx01', 'bx02']}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "TARGET_NAME = 'target'\n",
    "\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})\n",
    "\n",
    "print(\"Header: {}\".format(HEADER))\n",
    "print(\"Numeric Features: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "print(\"Categorical Features: {}\".format(CATEGORICAL_FEATURE_NAMES))\n",
    "print(\"Target: {}\".format(TARGET_NAME))\n",
    "print(\"Unused Features: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_features(features):\n",
    "    \n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y']) # features['x'] * features['y']\n",
    "    features['dist_xy'] =  tf.sqrt(tf.squared_difference(features['x'],features['y']))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_dataframe(dataset_df):\n",
    "    \n",
    "    dataset_df[\"x_2\"] = np.square(dataset_df['x'])\n",
    "    dataset_df[\"y_2\"] = np.square(dataset_df['y'])\n",
    "    dataset_df[\"xy\"] = dataset_df['x'] * dataset_df['y']\n",
    "    dataset_df['dist_xy'] =  np.sqrt(np.square(dataset_df['x']-dataset_df['y']))\n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_pandas_input_fn(file_name, mode=tf.estimator.ModeKeys.EVAL,\n",
    "                             skip_header_lines=0,\n",
    "                             num_epochs=1,\n",
    "                             batch_size=100):\n",
    "\n",
    "    df_dataset = pd.read_csv(file_name, names=HEADER, skiprows=skip_header_lines)\n",
    "    \n",
    "    x = df_dataset[FEATURE_NAMES].copy()\n",
    "    x = process_dataframe(x)\n",
    "    \n",
    "    y = df_dataset[TARGET_NAME]\n",
    "        \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    num_threads=1\n",
    "    \n",
    "    if multi_threading:\n",
    "        num_threads=multiprocessing.cpu_count()\n",
    "        num_epochs = int(num_epochs/num_threads) if mode == tf.estimator.ModeKeys.TRAIN else num_epochs\n",
    "    \n",
    "    pandas_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        batch_size=batch_size,\n",
    "        num_epochs= num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        target_column=TARGET_NAME\n",
    "    )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file: {}\".format(file_name))\n",
    "    print(\"Dataset size: {}\".format(len(df_dataset)))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return pandas_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/train-data.csv\n",
      "Dataset size: 12000\n",
      "Batch size: 100\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "Feature read from DataFrame: ['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']\n",
      "Target read from DataFrame: Tensor(\"fifo_queue_DequeueUpTo:9\", shape=(?,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "features, target = generate_pandas_input_fn(file_name=train_data_file)()\n",
    "print(\"Feature read from DataFrame: {}\".format(list(features.keys())))\n",
    "print(\"Target read from DataFrame: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'x': _NumericColumn(key='x', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y': _NumericColumn(key='y', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'x_2': _NumericColumn(key='x_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'y_2': _NumericColumn(key='y_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'xy': _NumericColumn(key='xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'dist_xy': _NumericColumn(key='dist_xy', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), 'alpha': _VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'beta': _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'alpha_X_beta': _CrossedColumn(keys=(_VocabularyListCategoricalColumn(key='alpha', vocabulary_list=('ax01', 'ax02'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='beta', vocabulary_list=('bx01', 'bx02'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=4, hash_key=None)}\n"
     ]
    }
   ],
   "source": [
    "def get_feature_columns():\n",
    "    \n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = ['x_2', 'y_2', 'xy', 'dist_xy']\n",
    "    all_numeric_feature_names = NUMERIC_FEATURE_NAMES + CONSTRUCTED_NUMERIC_FEATURES_NAMES\n",
    "\n",
    "    numeric_columns = {feature_name: tf.feature_column.numeric_column(feature_name)\n",
    "                       for feature_name in all_numeric_feature_names}\n",
    "\n",
    "    categorical_column_with_vocabulary = \\\n",
    "        {item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "        \n",
    "    feature_columns = {}\n",
    "\n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "        \n",
    "    \n",
    "    # apply feature extentions\n",
    "    \n",
    "    feature_columns['alpha_X_beta'] = tf.feature_column.crossed_column(\n",
    "        [feature_columns['alpha'], feature_columns['beta']], 4)\n",
    "    \n",
    "    return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "    \n",
    "    feature_columns = list(get_feature_columns().values())\n",
    "    \n",
    "    dense_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._NumericColumn),\n",
    "               feature_columns\n",
    "        )\n",
    "    )\n",
    "\n",
    "    categorical_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._VocabularyListCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._BucketizedColumn),\n",
    "                   feature_columns)\n",
    "    )\n",
    "\n",
    "    indicator_columns = list(\n",
    "            map(lambda column: tf.feature_column.indicator_column(column),\n",
    "                categorical_columns)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    estimator_feature_columns = dense_columns + indicator_columns \n",
    "    \n",
    "    estimator = tf.estimator.DNNRegressor(\n",
    "        \n",
    "        feature_columns= estimator_feature_columns,\n",
    "        hidden_units= hparams.hidden_units,\n",
    "        \n",
    "        optimizer= tf.train.AdamOptimizer(),\n",
    "        activation_fn= tf.nn.elu,\n",
    "        dropout= hparams.dropout_prob,\n",
    "        \n",
    "        config= run_config\n",
    "    )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Estimator Type: {}\".format(type(estimator)))\n",
    "    print(\"\")\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: trained_models/reg-model-01\n",
      "Hyper-parameters: [('batch_size', 500), ('dropout_prob', 0.0), ('hidden_units', [8, 4]), ('num_epochs', 100)]\n"
     ]
    }
   ],
   "source": [
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs = 100,\n",
    "    batch_size = 500,\n",
    "    hidden_units=[8, 4], \n",
    "    dropout_prob = 0.0)\n",
    "\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(model_name)\n",
    "\n",
    "run_config = tf.estimator.RunConfig().replace(model_dir=model_dir)\n",
    "print(\"Model directory: {}\".format(run_config.model_dir))\n",
    "print(\"Hyper-parameters: {}\".format(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/train-data.csv\n",
      "Dataset size: 12000\n",
      "Batch size: 500\n",
      "Epoch Count: 100\n",
      "Mode: train\n",
      "Thread Count: 1\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/reg-model-01', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1206126d8>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "Estimator Type: <class 'tensorflow.python.estimator.canned.dnn.DNNRegressor'>\n",
      "\n",
      "Estimator training started at 20:08:35\n",
      ".......................................\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into trained_models/reg-model-01/model.ckpt.\n",
      "INFO:tensorflow:loss = 135083.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 144.153\n",
      "INFO:tensorflow:loss = 136042.0, step = 101 (0.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.177\n",
      "INFO:tensorflow:loss = 142493.0, step = 201 (0.751 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.851\n",
      "INFO:tensorflow:loss = 151418.0, step = 301 (0.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.816\n",
      "INFO:tensorflow:loss = 139974.0, step = 401 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.446\n",
      "INFO:tensorflow:loss = 137613.0, step = 501 (0.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.752\n",
      "INFO:tensorflow:loss = 107903.0, step = 601 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.031\n",
      "INFO:tensorflow:loss = 118940.0, step = 701 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.3\n",
      "INFO:tensorflow:loss = 101067.0, step = 801 (0.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.461\n",
      "INFO:tensorflow:loss = 106191.0, step = 901 (0.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.317\n",
      "INFO:tensorflow:loss = 95948.4, step = 1001 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.89\n",
      "INFO:tensorflow:loss = 86084.6, step = 1101 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.279\n",
      "INFO:tensorflow:loss = 75202.7, step = 1201 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.85\n",
      "INFO:tensorflow:loss = 72829.7, step = 1301 (0.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.995\n",
      "INFO:tensorflow:loss = 69094.4, step = 1401 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.677\n",
      "INFO:tensorflow:loss = 65411.3, step = 1501 (0.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.224\n",
      "INFO:tensorflow:loss = 63431.3, step = 1601 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.625\n",
      "INFO:tensorflow:loss = 62667.9, step = 1701 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.575\n",
      "INFO:tensorflow:loss = 60012.7, step = 1801 (0.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.93\n",
      "INFO:tensorflow:loss = 67914.9, step = 1901 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.641\n",
      "INFO:tensorflow:loss = 66840.6, step = 2001 (0.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.279\n",
      "INFO:tensorflow:loss = 53918.1, step = 2101 (0.670 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.805\n",
      "INFO:tensorflow:loss = 48680.7, step = 2201 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.867\n",
      "INFO:tensorflow:loss = 63258.2, step = 2301 (0.496 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into trained_models/reg-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 59513.4.\n",
      ".......................................\n",
      "Estimator training finished at 20:08:54\n",
      "\n",
      "Estimator training elapsed time: 18.992184 seconds\n"
     ]
    }
   ],
   "source": [
    "train_input_fn = generate_pandas_input_fn(file_name= train_data_file, \n",
    "                                      mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                      num_epochs=hparams.num_epochs,\n",
    "                                      batch_size=hparams.batch_size) \n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "if not resume:\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "if train:\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Estimator training started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\")\n",
    "    \n",
    "    estimator.train(input_fn = train_input_fn)\n",
    "    \n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Estimator training finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Estimator training elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/test-data.csv\n",
      "Dataset size: 5000\n",
      "Batch size: 5000\n",
      "Epoch Count: 1\n",
      "Mode: eval\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-02-20:08:54\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-02-20:08:55\n",
      "INFO:tensorflow:Saving dict for global step 2400: average_loss = 119.654, global_step = 2400, loss = 598271.0\n",
      "\n",
      "{'average_loss': 119.65427, 'loss': 598271.38, 'global_step': 2400}\n",
      "\n",
      "RMSE: 10.93866\n"
     ]
    }
   ],
   "source": [
    "test_size = 5000\n",
    "\n",
    "test_input_fn = generate_pandas_input_fn(file_name=test_data_file, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= test_size)\n",
    "\n",
    "results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "print(\"\")\n",
    "print(results)\n",
    "rmse = round(math.sqrt(results[\"average_loss\"]),5)\n",
    "print(\"\")\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file: data/test-data.csv\n",
      "Dataset size: 5000\n",
      "Batch size: 5\n",
      "Epoch Count: 1\n",
      "Mode: infer\n",
      "Thread Count: 1\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "[43.802998, -6.4944687, 10.617713, 2.1373863, 0.59306604]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "predict_input_fn = generate_pandas_input_fn(file_name=test_data_file, \n",
    "                                      mode= tf.estimator.ModeKeys.PREDICT,\n",
    "                                      batch_size= 5)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn)\n",
    "print((list(map(lambda item: item[\"predictions\"][0],list(itertools.islice(predictions, 5))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save & Serve the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-01/model.ckpt-2400\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"trained_models/reg-model-01/export/temp-b'1509653337'/saved_model.pb\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'trained_models/reg-model-01/export/1509653337'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_dir = model_dir + \"/export\"\n",
    "\n",
    "def process_features(features):\n",
    "    \n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y'])\n",
    "    features['dist_xy'] =  tf.sqrt(tf.squared_difference(features['x'],features['y']))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def csv_serving_input_fn():\n",
    "    \n",
    "    SERVING_HEADER = ['x','y','alpha','beta']\n",
    "    SERVING_HEADER_DEFAULTS = [[0.0], [0.0], ['NA'], ['NA']]\n",
    "\n",
    "    rows_string_tensor = tf.placeholder(dtype=tf.string,\n",
    "                                         shape=[None],\n",
    "                                         name='csv_rows')\n",
    "    \n",
    "    receiver_tensor = {'csv_rows': rows_string_tensor}\n",
    "\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=SERVING_HEADER_DEFAULTS)\n",
    "    features = dict(zip(SERVING_HEADER, columns))\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        process_features(features), receiver_tensor)\n",
    "\n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base = export_dir,\n",
    "    serving_input_receiver_fn = csv_serving_input_fn,\n",
    "    as_text=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\n",
      "variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.ml-engine.local.predict) RuntimeError: Bad magic number in .pyc file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME='reg-model-01'\n",
    "LAST=$(ls trained_models/${MODEL_NAME}/export | tail -1)\n",
    "SAVE_MODEL_DIR=trained_models/$MODEL_NAME/export/$LAST\n",
    "ls $SAVE_MODEL_DIR\n",
    "\n",
    "gcloud ml-engine local predict --model-dir=$SAVE_MODEL_DIR --text-instances='data/new-data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we improve?\n",
    "\n",
    "* **Use data files instead of  DataFrames** - pandas dataframes need to fit in memory, and hard to distribute. Working with (sharded) training data files allows reading records in batches (so we can work with large data set regardless the memory size), as well as supporting distributed training (data parallelism).\n",
    "\n",
    "\n",
    "* **Use Experiment APIs** - Experiment API knows how to invoke training and eval loops in a sensible fashion for local & distributed training.\n",
    "\n",
    "\n",
    "* ** Early Stopping** - Use the validation set evaluation to stop the training and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
