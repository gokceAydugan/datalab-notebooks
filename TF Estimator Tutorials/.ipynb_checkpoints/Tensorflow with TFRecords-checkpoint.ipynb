{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import glob\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_classes=4,n_features=20,n_clusters_per_class=2,n_informative=15,n_samples=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = 'f01,f02,f03,f04,f05,f06,f07,f08,f09,f10,f11,f12,f13,f14,f15,f16,f17,f18,f19,f20,target'.split(',')\n",
    "len(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data_files(partitions_count, pattern):\n",
    "    for i in range(partitions_count):\n",
    "        partition = data.sample(frac=0.15)\n",
    "        partition.to_csv(\"data/gen-{}-{}.csv\".format(pattern,str(i+1).zfill(2)),header=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_data_files(10,\"train\")\n",
    "save_data_files(5,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADER = 'key,f01,f02,f03,f04,f05,f06,f07,f08,f09,f10,f11,f12,f13,f14,f15,f16,f17,f18,f19,f20,target'.split(',')\n",
    "DATA_TYPES = ['int'] + (['float']*20) + ['string']\n",
    "DEFAULTS = [[1]]+([[0.0]]*20)+[['NA']] \n",
    "NUMERIC_FEATURE_NAMES = 'f01,f02,f03,f04,f05,f06,f07,f08,f09,f10,f11,f12,f13,f14,f15,f16,f17,f18,f19,f20'.split(',')\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES \n",
    "KEY_COLUMN_NAME = 'key'\n",
    "TARGET_NAME = 'target'\n",
    "TARGET_LABELS = '1,2,3,4'.split(',')\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def get_tfrecord_feature(i,value):\n",
    "    data_type = DATA_TYPES[i]\n",
    "    if data_type == \"int\":\n",
    "        return  _int_feature(int(value))\n",
    "    elif data_type == \"float\":\n",
    "        return _float_feature(float(value))\n",
    "    else:\n",
    "        return _bytes_feature(value.tostring())\n",
    "    \n",
    "def get_tfrecord_features(df_row):\n",
    "    \n",
    "    tf_features = {HEADER[i]: get_tfrecord_feature(i,df_row[i])\n",
    "                       for i in range(len(HEADER))}\n",
    "    return tf_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataframe(pattern):\n",
    "\n",
    "    files = glob.glob(\"data/gen-{}-*.csv\".format(pattern))\n",
    "\n",
    "    df_list = []\n",
    "    for file_path in files:\n",
    "        df = pd.read_csv(file_path, index_col=None, header=None)\n",
    "        df_list.append(df)\n",
    "\n",
    "    frame = pd.DataFrame()\n",
    "    frame = pd.concat(df_list)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_tfrecords(dataframe,pattern,partition):\n",
    "    print(len(dataframe))\n",
    "    with tf.python_io.TFRecordWriter(\"data/tf-{}-{}.tfr\".format(pattern,partition)) as writer:\n",
    "        for row in dataframe.values:\n",
    "            fd = get_tfrecord_features(row)\n",
    "            #print(fd)\n",
    "            example = tf.train.Example(\n",
    "                features = tf.train.Features(feature=fd)\n",
    "            \n",
    "            )\n",
    "#             example.features.feature[\"features\"].float_list.value.extend(features)\n",
    "#             example.features.feature[\"label\"].bytes_list.value.append(str(label))\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_dataframe(\"train\")\n",
    "len(train_data.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data_as_tfrecords(dataframe, pattern, partition_count):\n",
    "    batch_size = len(dataframe)/partition_count\n",
    "    for i in range(partition_count):\n",
    "        start_index = (i*partition_count)\n",
    "        end_index = (i*partition_count) + batch_size\n",
    "        if end_index > len(dataframe) - start_index:\n",
    "            end_index = len(dataframe) - start_index\n",
    "            \n",
    "        convert_to_tfrecords(dataframe.iloc[int(start_index): int(end_index),:],pattern,str(i+1).zfill(2))\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "0\n",
      "15000\n",
      "1\n",
      "15000\n",
      "2\n",
      "15000\n",
      "3\n",
      "15000\n",
      "4\n",
      "15000\n",
      "5\n",
      "15000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/khalidsalama/anaconda/lib/python3.6/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'f14'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-1b2f7ee0095f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_data_as_tfrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-11a758ff36e0>\u001b[0m in \u001b[0;36msave_data_as_tfrecords\u001b[0;34m(dataframe, pattern, partition_count)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mend_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mconvert_to_tfrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-6be99bb6c5b1>\u001b[0m in \u001b[0;36mconvert_to_tfrecords\u001b[0;34m(dataframe, pattern, partition)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m#print(fd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             example = tf.train.Example(\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             )\n",
      "\u001b[0;32m/Users/khalidsalama/anaconda/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m           \u001b[0mnew_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m           \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m           \u001b[0m_ReraiseTypeErrorWithFieldName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/khalidsalama/anaconda/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1221\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m           \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0mfield_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/khalidsalama/anaconda/lib/python3.6/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;31m# self._message_listener.Modified() not required here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;31m# mutations to submessages already propagate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/khalidsalama/anaconda/lib/python3.6/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m       \u001b[0mnew_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/khalidsalama/anaconda/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size_dirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_data_as_tfrecords(train_data,\"train\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = load_dataframe(\"test\")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_label_column(label_string_tensor):\n",
    "    table = tf.contrib.lookup.index_table_from_tensor(tf.constant(TARGET_LABELS))\n",
    "    return table.lookup(label_string_tensor)\n",
    "\n",
    "def csv_input_fn(filename, num_epochs=None, batch_size=512):\n",
    "    \n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    " \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=num_epochs, shuffle=False)\n",
    "    \n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "    \n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    features.pop(KEY_COLUMN_NAME)\n",
    "    \n",
    "    target = features.pop(TARGET_NAME)    \n",
    "    \n",
    "    return features, parse_label_column(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_classifier(run_config,hparams):\n",
    "  \n",
    "    numeric_columns = [tf.feature_column.numeric_column(feature_name)\n",
    "                       for feature_name in NUMERIC_FEATURE_NAMES]\n",
    "\n",
    "    dnn_optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    estimator = tf.contrib.learn.DNNClassifier(\n",
    "                feature_columns = numeric_columns,\n",
    "                optimizer=dnn_optimizer,\n",
    "                hidden_units=hparams.hidden_units,\n",
    "                config = run_config\n",
    "                )\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_files = \"data/gen-train-*.csv\"\n",
    "eval_data_files = \"data/gen-test-*.csv\"\n",
    "\n",
    "def create_experiment(run_config,hparams):\n",
    "    \n",
    "    estimator = create_classifier(run_config, hparams)\n",
    "    \n",
    "    evaluation_metrics={\n",
    "    'accuracy': tf.contrib.learn.MetricSpec(\n",
    "        metric_fn=tf.metrics.accuracy, \n",
    "        prediction_key=\"classes\",\n",
    "        label_key=None)\n",
    "    }\n",
    "    \n",
    "    experiment =  tf.contrib.learn.Experiment(estimator = estimator, \n",
    "                                     train_steps = hparams.training_steps,\n",
    "                                     train_input_fn = lambda: csv_input_fn(train_data_files,\n",
    "                                                                           num_epochs=hparams.num_epochs,\n",
    "                                                                           batch_size = hparams.batch_size\n",
    "                                                                          ), \n",
    "                                     eval_input_fn =lambda: csv_input_fn(eval_data_files),\n",
    "                                     eval_metrics = evaluation_metrics\n",
    "                                    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Set params\n",
    "hparams  = tf.contrib.training.HParams(training_steps=None,\n",
    "                                       num_epochs = 10,\n",
    "                                       batch_size = 5000,\n",
    "                                       hidden_units=[16, 8])\n",
    "\n",
    "\n",
    "local_model_dir = \"trained_models/demo_classifier\"\n",
    "shutil.rmtree(local_model_dir, ignore_errors=True)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    model_dir=local_model_dir\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.contrib.learn.learn_runner.run(experiment_fn = create_experiment, \n",
    "                               run_config = run_config,\n",
    "                               schedule=\"train_and_evaluate\",\n",
    "                               hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
